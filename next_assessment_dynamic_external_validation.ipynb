{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ee64f6-f1c4-4660-88d0-1d3bfe60c45e",
   "metadata": {},
   "source": [
    "### External validation \n",
    "\n",
    "DATA: NKI\n",
    "\n",
    "1) Read external data set\n",
    "2) Include Thresholds \n",
    "3) Preprocess external data set as in next_assessment_dynamic_preprocessing.py\n",
    "4) Validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a16ab-6795-4655-8be1-458a04bb437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# Load the external validation dataset\n",
    "df = pd.read_csv('data/nki_validation.csv', sep=\",\", decimal=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a17d31-70fe-4838-b1c5-24d935755639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cols_to_convert = [\n",
    "    \"C30_PF2\", \"C30_RF2\", \"C30_SF\", \"C30_EF\", \"C30_CF\", \n",
    "    \"C30_FA\", \"C30_PA\", \"C30_NV\", \"C30_SL\", \"C30_DY\", \n",
    "    \"C30_AP\", \"C30_CO\", \"C30_DI\", \"C30_FI\"\n",
    "]\n",
    "\n",
    "df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Apply conditions after conversion\n",
    "df1 = df.assign(\n",
    "    C30_PF2_class=np.where(df['C30_PF2'] < 83, 1, 0),  # Physical functioning\n",
    "    C30_RF2_class=np.where(df['C30_RF2'] < 58, 1, 0),  # Role functioning\n",
    "    C30_SF_class=np.where(df['C30_SF'] < 58, 1, 0),    # Social functioning\n",
    "    C30_EF_class=np.where(df['C30_EF'] < 71, 1, 0),    # Emotional functioning\n",
    "    C30_CF_class=np.where(df['C30_CF'] < 75, 1, 0),    # Cognitive functioning\n",
    "    C30_FA_class=np.where(df['C30_FA'] > 39, 1, 0),    # Fatigue\n",
    "    C30_PA_class=np.where(df['C30_PA'] > 25, 1, 0),    # Pain\n",
    "    C30_NV_class=np.where(df['C30_NV'] > 8, 1, 0),     # Nausea\n",
    "    C30_SL_class=np.where(df['C30_SL'] > 50, 1, 0),    # Sleep disturbances\n",
    "    C30_DY_class=np.where(df['C30_DY'] > 17, 1, 0),    # Dyspnea\n",
    "    C30_AP_class=np.where(df['C30_AP'] > 50, 1, 0),    # Appetite loss\n",
    "    C30_CO_class=np.where(df['C30_CO'] > 50, 1, 0),    # Constipation\n",
    "    C30_DI_class=np.where(df['C30_DI'] > 17, 1, 0),    # Diarrhea\n",
    "    C30_FI_class=np.where(df['C30_FI'] > 17, 1, 0)     # Financial impact\n",
    ")\n",
    "\n",
    "df1.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb52da5-5527-4fe8-9a13-faf4e9df5a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows without assessment date\n",
    "df2 = df1.dropna(subset=['Assessment_date_days'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fcc2f0-c84c-44a6-974b-7247a927187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all patients that have only one assessment date\n",
    "assessment_counts = df2.groupby('BALANCE_ID').count()['Pat_num']\n",
    "patients_with_two_assessments = assessment_counts[assessment_counts > 1].index.values\n",
    "df3 = df2[df2.BALANCE_ID.isin(patients_with_two_assessments)]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7a06f-6d03-43df-8358-026e094262a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoded = [\n",
    "        \"Marital_status\",\n",
    "        \"Education_status\",\n",
    "        \"Diag_code\",\n",
    "        \"Lateralisation\",\n",
    "        \"Location_code\",\n",
    "        \"Histology_code\",\n",
    "        \"Differentation_grade\",\n",
    "        'Country',\n",
    "        'T_staging',\n",
    "        'N_staging',\n",
    "        \"Stage\",\n",
    "        \"Her2Neu_status\",\n",
    "        \"T_staging_postOP\",\n",
    "        \"N_staging_postOP\",\n",
    "        \"Stage_postOP\",\n",
    "        'Treatment_1', 'Treatment_2', 'Treatment_3', 'Treatment_4', 'Treatment_5', 'Treatment_6', \n",
    "        'Treatment_7', 'Treatment_8', 'Treatment_9', 'Treatment_10', 'Treatment_11', 'Treatment_12'\n",
    "]\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "raw_data = ohe.fit_transform(df3[one_hot_encoded]).todense()\n",
    "one_hot_column_names = ohe.get_feature_names_out(one_hot_encoded)\n",
    "df_one_hot = pd.DataFrame(raw_data, columns=one_hot_column_names)\n",
    "df_one_hot.index = df3.index\n",
    "df4 = pd.concat([df3.drop(columns=one_hot_encoded), df_one_hot], axis=1)\n",
    "df4['Source'] = 2\n",
    "df4.columns = df4.columns.str.replace(r'\\.0$', '', regex=True)\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2129d57-be7d-468a-8d2c-c5b688c5ed56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_columns = pd.read_pickle('data/cached_df.pckl')\n",
    "df5 = pd.concat([df_all_columns, df4])\n",
    "df5 = df5[df5.Source == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93b607-e1e2-4f09-a38e-85e7b15002c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import targets, get_feature_sets\n",
    "\n",
    "feature_sets = get_feature_sets(df5)\n",
    "# Sort the data to ensure temporal ordering within each BALANCE_ID\n",
    "validation_data = df5.dropna(subset=targets).sort_values(by=['BALANCE_ID', 'Assessment_date_days'])\n",
    "\n",
    "Xs, ys = [], []\n",
    "lastrow = None\n",
    "\n",
    "patient_ids = df5.BALANCE_ID.unique()\n",
    "for patient_id in patient_ids:\n",
    "    pdf = df5[df5.BALANCE_ID == patient_id]\n",
    "    for i in range(0, pdf.shape[0] - 1):\n",
    "        for j in range(i + 1, pdf.shape[0]):\n",
    "            assessment0 = pdf.iloc[i].copy()\n",
    "            assessment1 = pdf.iloc[j]\n",
    "\n",
    "            time_diff = assessment1['Assessment_date_days'] - assessment0['Assessment_date_days']\n",
    "            assessment0['time_diff'] = time_diff\n",
    "            Xs.append(assessment0)\n",
    "            ys.append(assessment1[['BALANCE_ID'] + targets])\n",
    "            \n",
    "# Convert to DataFrames\n",
    "X = pd.DataFrame(Xs).reset_index(drop=True)\n",
    "y = pd.DataFrame(ys).reset_index(drop=True)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bcb9a-cede-416b-bd72-7e13e5af5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_pickle('data/external_x.pckl')\n",
    "y.to_pickle('data/external_y.pckl')\n",
    "\n",
    "y.to_csv('data/targets_external.csv')\n",
    "X.to_csv('data/predictors_external.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da45747-d733-42c7-b556-bbef8ed5db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[\"BALANCE_ID\"].nunique())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c3b19-5652-48da-aed6-10768c5d2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67467bee-926a-4565-a181-29f9bb37c874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract features and ensure only the required features are used\n",
    "# Ensure only the features that exist in the validation_data are selected\n",
    "valid_features = [feature for feature in feature_sets['all'] if feature in validation_data.columns]\n",
    "valid_features.append('Source')\n",
    "features = X[valid_features]  # Select only valid features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716e3de-401e-4380-ae55-6220d0b41f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, accuracy_score, balanced_accuracy_score, \n",
    "    recall_score, average_precision_score, brier_score_loss\n",
    ")\n",
    "\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Initialize a dictionary for the results\n",
    "results = {}\n",
    "\n",
    "with PdfPages(\"calibration_plots.pdf\") as pdf:\n",
    "    # Iterate through each target in the targets list\n",
    "    for target in targets:\n",
    "    \n",
    "        # Load the model from the file\n",
    "        with open(f'calibrated_model_{target}.pkl', 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "        \n",
    "        print(f\"Validating for target: {target}\")\n",
    "    \n",
    "        # Ensure the target column exists in the validation dataset\n",
    "        if target not in validation_data.columns:\n",
    "            print(f\"Warning: Target '{target}' not found in the dataset. Skipping...\\n\")\n",
    "            continue\n",
    "    \n",
    "        y_true = y[target]  # Extract the target column\n",
    "    \n",
    "        # Validate the model\n",
    "        y_pred = model.predict(features)\n",
    "        \n",
    "        y_proba = model.predict_proba(features)[:, 1]  # Probability for the positive class\n",
    "    \n",
    "        boot_metrics = {\n",
    "            'roc_auc': [],\n",
    "            'f1': [],\n",
    "            'f1_weighted': [],\n",
    "            'accuracy': [],\n",
    "            'balanced_accuracy': [],\n",
    "            'recall_weighted': [],\n",
    "            'average_precision': [],\n",
    "            'brier_score_loss': [],\n",
    "            'calibration_slope': [],\n",
    "            'calibration_intercept': []\n",
    "        }\n",
    "        \n",
    "        # Initialize a dictionary for this target\n",
    "        results[target] = {}\n",
    "    \n",
    "        for i in range(n_bootstraps):\n",
    "            indices = rng.choice(range(len(y_true)), size=len(y_true), replace=True)\n",
    "            y_true_bs = y_true.iloc[indices]\n",
    "            y_pred_bs = y_pred[indices]\n",
    "            y_proba_bs = y_proba[indices]\n",
    "        \n",
    "            try:\n",
    "                boot_metrics['roc_auc'].append(roc_auc_score(y_true_bs, y_proba_bs))\n",
    "                boot_metrics['f1'].append(f1_score(y_true_bs, y_pred_bs))\n",
    "                boot_metrics['f1_weighted'].append(f1_score(y_true_bs, y_pred_bs, average='weighted'))\n",
    "                boot_metrics['accuracy'].append(accuracy_score(y_true_bs, y_pred_bs))\n",
    "                boot_metrics['balanced_accuracy'].append(balanced_accuracy_score(y_true_bs, y_pred_bs))\n",
    "                boot_metrics['recall_weighted'].append(recall_score(y_true_bs, y_pred_bs, average='weighted'))\n",
    "                boot_metrics['average_precision'].append(average_precision_score(y_true_bs, y_proba_bs))\n",
    "                boot_metrics['brier_score_loss'].append(brier_score_loss(y_true_bs, y_proba_bs))\n",
    "                \n",
    "                # Compute calibration slope and intercept:\n",
    "                # Clip probabilities to avoid numeric issues with the logit transformation\n",
    "                y_proba_bs_clipped = np.clip(y_proba_bs, 1e-15, 1-1e-15)\n",
    "                logits = np.log(y_proba_bs_clipped / (1 - y_proba_bs_clipped))\n",
    "                # Use linear regression (np.polyfit returns [slope, intercept] when degree=1)\n",
    "                slope, intercept = np.polyfit(logits, y_true_bs, 1)\n",
    "                boot_metrics['calibration_slope'].append(slope)\n",
    "                boot_metrics['calibration_intercept'].append(intercept)\n",
    "                \n",
    "            except ValueError:\n",
    "                # Happens if a bootstrap sample contains only one class\n",
    "                continue\n",
    "    \n",
    "        def ci_bounds(metric_list):\n",
    "            return np.percentile(metric_list, [2.5, 97.5])\n",
    "        \n",
    "        results[target]['External Dataset (NKI)'] = {}\n",
    "        for metric, scores in boot_metrics.items():\n",
    "            if len(scores) > 0:\n",
    "                mean_val = np.mean(scores)\n",
    "                ci_low, ci_high = ci_bounds(scores)\n",
    "                results[target]['External Dataset (NKI)'][metric] = {\n",
    "                    'mean': mean_val,\n",
    "                    '95% CI': (ci_low, ci_high)\n",
    "                }\n",
    "    \n",
    "        # Compute calibration curve\n",
    "        prob_true, prob_pred = calibration_curve(y_true, y_proba, n_bins=10)\n",
    "    \n",
    "        # Plot calibration curve \n",
    "        fig, ax = plt.subplots(figsize=(7, 6))\n",
    "        ax.plot(prob_pred, prob_true, marker='o', label=\"Model Calibration\")\n",
    "        ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Perfect Calibration\")\n",
    "        ax.set_xlabel(\"Mean Predicted Probability\")\n",
    "        ax.set_ylabel(\"Fraction of Positives\")\n",
    "        ax.set_title(f\"Calibration Plot: {target}\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Save the current figure into the PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d1635-6a11-4c57-b6d0-001ec8428bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create result export\n",
    "\n",
    "rows = []\n",
    "\n",
    "for target, classifiers in results.items():\n",
    "    for classifier_name, metrics in classifiers.items():\n",
    "        row = {\n",
    "            'Target': target,\n",
    "            'Classifier': classifier_name\n",
    "        }\n",
    "\n",
    "        for metric_name, value in metrics.items():\n",
    "            if isinstance(value, dict) and 'mean' in value and '95% CI' in value:\n",
    "                # It's a nested dictionary with mean and confidence interval\n",
    "                row[f\"{metric_name}\"] = round(value['mean'], 3)\n",
    "                row[f\"{metric_name}_ci_low\"] = round(value['95% CI'][0], 3)\n",
    "                row[f\"{metric_name}_ci_high\"] = round(value['95% CI'][1], 3)\n",
    "            elif isinstance(value, list) and len(value) == 1:\n",
    "                # Single value stored as a list\n",
    "                row[metric_name] = round(value[0], 3)\n",
    "            else:\n",
    "                # Fallback for unexpected structure\n",
    "                row[metric_name] = value\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(rows)\n",
    "\n",
    "# Display\n",
    "display(results_df)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('results_external_validation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
